{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate syntactic accuracy and semantic accuracy\n",
    "\n",
    " contents\n",
    "- load data \n",
    "- Model\n",
    "    - skipgram\n",
    "    - skipgram(neg)\n",
    "    - cbow\n",
    "    - Glove\n",
    "- syntactic accuracy and semantic accuracy Result\n",
    "- correlation\n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load text line by line\n",
    "with open(\"questions-words.txt\") as f:\n",
    "    contents = f.read().splitlines() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "capital-common-countries 506\n",
      "capital-world 4524\n",
      "currency 866\n",
      "city-in-state 2467\n",
      "family 506\n",
      "gram1-adjective-to-adverb 992\n",
      "gram2-opposite 812\n",
      "gram3-comparative 1332\n",
      "gram4-superlative 1122\n",
      "gram5-present-participle 1056\n",
      "gram6-nationality-adjective 1599\n",
      "gram7-past-tense 1560\n",
      "gram8-plural 1332\n",
      "gram9-plural-verbs 870\n"
     ]
    }
   ],
   "source": [
    "# convert all data in question.txt to dictionary\n",
    "\n",
    "group_contents = []\n",
    "dict_contents = {}\n",
    "for line in contents:\n",
    "    if ':' in line:\n",
    "        curent_content = line.replace(':',\"\").strip()\n",
    "        dict_contents[curent_content]=[]\n",
    "    else:\n",
    "        dict_contents[curent_content].append(line.split(\" \"))\n",
    "\n",
    "for key in dict_contents.keys():\n",
    "    print(key,len(dict_contents[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pick all data\n",
    "# analogy = {\"syntactic\":[],\"semantic\":[]}\n",
    "# #syntactic accuracy and semantic \n",
    "# for key in dict_contents.keys():\n",
    "#     if \"gram\" in key:\n",
    "#         analogy[\"syntactic\"].extend(dict_contents[key])\n",
    "#     else:\n",
    "#         analogy[\"semantic\"].extend(dict_contents[key]) \n",
    "\n",
    "# for key in analogy.keys():\n",
    "#     print(key,len(analogy[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "syntactic 2591\n",
      "semantic 3479\n"
     ]
    }
   ],
   "source": [
    "# pick some data\n",
    "analogy = {\"syntactic\":[],\"semantic\":[]}\n",
    "analogy[\"syntactic\"] = dict_contents[\"gram1-adjective-to-adverb\"] + dict_contents[\"gram6-nationality-adjective\"]\n",
    "analogy[\"semantic\"] = dict_contents[\"family\"] + dict_contents[\"capital-common-countries\"] + dict_contents[\"city-in-state\"]\n",
    "for key in analogy.keys():\n",
    "    print(key,len(analogy[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add more data\n",
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file which contain spotify song lyric \n",
    "df = pd.read_csv(\"C:\\\\Users\\\\ASUS\\\\My_Journal\\\\Text\\\\My-NLP\\\\spotify_millsongdata.csv\")\n",
    "\n",
    "# Randomly select 5 song\n",
    "sample = df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "corpus = sample[\"text\"]\n",
    "#load \n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# reduce space , lower all character and use spacy to tokenize\n",
    "sparcy_tokenized = [nlp((' '.join(lyric.split())).lower()) for lyric in corpus]\n",
    "# convert scapy token to str\n",
    "corpus_tokenized = [[str(word) for word in sublist] for sublist in sparcy_tokenized]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "6075\n"
     ]
    }
   ],
   "source": [
    "print(len(corpus_tokenized))\n",
    "# combine all\n",
    "corpus_tokenized = corpus_tokenized+analogy[\"semantic\"]+analogy[\"syntactic\"]\n",
    "len(corpus_tokenized)\n",
    "print(len(corpus_tokenized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use all question data\n",
    "# corpus_tokenized =[]\n",
    "\n",
    "# for key in dict_contents.keys():\n",
    "#     corpus_tokenized+=dict_contents[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # use some data of question-word\n",
    "# corpus_tokenized = dict_contents[\"family\"]+dict_contents[\"gram1-adjective-to-adverb\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we want to flatten this (basically merge all list)\n",
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. numericalize\n",
    "\n",
    "\n",
    "\n",
    "def numericalize_str(corpus_tokenized):\n",
    "\n",
    "    #2.1 get all the unique words\n",
    "\n",
    "    vocabs  = list(set(flatten(corpus_tokenized)))  #vocabs is a term defining all unique words your system know\n",
    "\n",
    "    #2.2 assign id to all these vocabs\n",
    "    word2index = {v: idx for idx, v in enumerate(vocabs)}\n",
    "\n",
    "    #add <UNK>, which is a very normal token exists in the world\n",
    "    vocabs.append('<UNK>') #chaky, can it be ##UNK, or UNKKKKKK, or anything\n",
    "\n",
    "    #now we have a way to know what is the id of <UNK>\n",
    "    word2index['<UNK>'] = len(word2index)  #usually <UNK> is 0\n",
    "\n",
    "    #create index2word dictionary\n",
    "    #2 min    \n",
    "    index2word = {v:k for k, v in word2index.items()}\n",
    "\n",
    "    return vocabs,word2index,index2word\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabs,word2index,index2word = numericalize_str(corpus_tokenized) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import inspect\n",
    "\n",
    "def check_local_variables(func):\n",
    "    # parse the function's code\n",
    "    source_lines, _ = inspect.getsourcelines(func)\n",
    "    source_code = '\\n'.join(source_lines)\n",
    "    tree = ast.parse(source_code)\n",
    "    # keep track of the local variables\n",
    "    local_vars = set(func.__code__.co_varnames)\n",
    "    # traverse the AST\n",
    "    for node in ast.walk(tree):\n",
    "        if isinstance(node, ast.Name) and node.id not in local_vars:\n",
    "            print(f\"Variable {node.id} is not local.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, word2index):\n",
    "    #map(function, list of something)\n",
    "    #map will look at each of element in this list, and apply this function\n",
    "    idxs = list(map(lambda w: word2index[w] if word2index.get(w) is not None else word2index[\"<UNK>\"], seq))\n",
    "    return torch.LongTensor(idxs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count all the occurrences of vocabs\n",
    "from collections import Counter\n",
    "\n",
    "def unigram(corpus_tokenized,vocabs,z = 0.001):\n",
    "    # Unigram\n",
    "    \n",
    "    word_count = Counter(flatten(corpus_tokenized))\n",
    "    # word_count\n",
    "    num_total_words = sum([c for w, c in word_count.items()])\n",
    "    print(\"total word : \",num_total_words)\n",
    "\n",
    "    unigram_table = []\n",
    "\n",
    "    for v in vocabs:\n",
    "        uw = word_count[v]/num_total_words\n",
    "        uw_alpha = uw ** 0.75\n",
    "        uw_alpha_dividebyz = int(uw_alpha / z)\n",
    "        # print(\"vocab: \", v)\n",
    "        # print(\"distribution: \", uw_alpha_dividebyz)\n",
    "        unigram_table.extend([v] * uw_alpha_dividebyz)\n",
    "    \n",
    "    return unigram_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "#you don't want to pick samples = targets, basically negative samples\n",
    "#k = number of negative samples - how many? they found 10 is the best\n",
    "#will be run during training\n",
    "#after random_batch, \n",
    "def negative_sampling(targets, unigram_table, k):\n",
    "    #targets is already in id.....\n",
    "    #but the unigram_table is in word....\n",
    "    #1. get the batch size of this targets\n",
    "    batch_size = targets.shape[0]\n",
    "    neg_samples = []\n",
    "    #2. for each batch\n",
    "    for i in range(batch_size):\n",
    "        #randomly pick k negative words from unigram_table\n",
    "        target_index = targets[i].item()  #looping each of the batch....\n",
    "        nsample = []\n",
    "        while len(nsample) < k:\n",
    "            neg = random.choice(unigram_table)\n",
    "            #if this word == target, skip this word\n",
    "            if word2index[neg] == target_index:\n",
    "                continue\n",
    "            nsample.append(neg)\n",
    "        #append this word to some list\n",
    "        neg_samples.append(prepare_sequence(nsample, word2index).reshape(1, -1))  #tensor[], tensor[]\n",
    "    return torch.cat(neg_samples)  #tensor[[], []]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_data(corpus_tokenized,mode,window_size=2):\n",
    "\n",
    "    pairs = []\n",
    "\n",
    "    #for each corpus\n",
    "    for sent in corpus_tokenized:\n",
    "        #for each sent [\"apple\", \"banana\", \"fruit\"]\n",
    "        # # start from window_size end at window_size before last\n",
    "        for i in range(window_size,len(sent)-window_size):\n",
    "            center_word = word2index[sent[i]]\n",
    "            # outside words \n",
    "            outside_words = [word2index[sent[j]] for j in range(max(0, i - window_size), min(len(sent), i + window_size + 1)) if j != i]\n",
    "            for o in outside_words:\n",
    "                if mode == \"skipgram\":\n",
    "                    # append outside word as input center word as output\n",
    "                    pairs.append([center_word,o])\n",
    "                elif mode == \"cbow\":\n",
    "                    pairs.append([o,center_word])\n",
    "    \n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_batch(batch_size, pair, window_size=2):\n",
    "                 \n",
    "    #only get a batch, not the entire list\n",
    "    random_index = np.random.choice(range(len(pair)), batch_size, replace=False)\n",
    "             \n",
    "    #appending some list of inputs and labels\n",
    "    random_inputs, random_labels = [], []   \n",
    "    for index in random_index:\n",
    "        random_inputs.append([pair[index][0]])  #outside words, this will be a shape of (1, ) --> (1, 1) for modeling\n",
    "        random_labels.append([pair[index][1]])\n",
    "        \n",
    "    return np.array(random_inputs), np.array(random_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighting(w_i, w_j, X_ik):   #why we need w_i and w_j, because we can try its co-occurrences, if it's too big, we scale it down\n",
    "    \n",
    "    #check whether the co-occurrences between these two word exists???\n",
    "    try:\n",
    "        x_ij = X_ik[(w_i, w_j)]\n",
    "    except:\n",
    "        x_ij = 1  #why one, so that the probability thingy won't break...(label smoothing)\n",
    "        \n",
    "    #maximum co-occurrences; we follow the paper\n",
    "    x_max = 100\n",
    "    alpha = 0.75\n",
    "    \n",
    "    #if the co-occurrences does not exceed x_max, scale it down based on some alpha\n",
    "    if x_ij < x_max:\n",
    "        result = (x_ij/x_max) ** alpha\n",
    "    else:\n",
    "        result = 1 #this is the maximum probability you can have\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations_with_replacement\n",
    "\n",
    "def wdic_co(vocabs,corpus_tokenized,window_size = 2): \n",
    "\n",
    "    skip_grams = []\n",
    "    #for each corpus\n",
    "    for sent in corpus_tokenized:\n",
    "        #for each sent [\"apple\", \"banana\", \"fruit\"]\n",
    "        # # start from window_size end at window_size before last\n",
    "        for i in range(window_size,len(sent)-window_size):\n",
    "            center_word = sent[i]\n",
    "            # outside words \n",
    "            outside_words = [sent[j] for j in range(max(0, i - window_size), min(len(sent), i + window_size + 1)) if j != i]\n",
    "            for o in outside_words:\n",
    "                # append outside word as input center word as output\n",
    "                skip_grams.append((center_word,o))\n",
    "\n",
    "    X_ik_skipgram = Counter(skip_grams)\n",
    "\n",
    "    X_ik = {} #for keeping the co-occurrences\n",
    "    weighting_dic = {} #for keeping all the probability after passing through the weighting function\n",
    "\n",
    "    for bigram in combinations_with_replacement(vocabs, 2):  #we need to also think its reverse\n",
    "        #if this bigram exists in X_ik_skipgrams\n",
    "        #we gonna add this to our co-occurence matrix\n",
    "        if X_ik_skipgram.get(bigram) is not None:\n",
    "            cooc = X_ik_skipgram[bigram]  #get the co-occurrence\n",
    "            X_ik[bigram] = cooc + 1 #this is again basically label smoothing....(stability issues (especially when divide something))\n",
    "            X_ik[(bigram[1], bigram[0])] = cooc + 1  #trick to get all pairs\n",
    "\n",
    "        \n",
    "        #apply the weighting function using this co-occurrence matrix thingy    \n",
    "        weighting_dic[bigram] = weighting(bigram[0], bigram[1], X_ik)\n",
    "        weighting_dic[(bigram[1], bigram[0])] = weighting(bigram[1], bigram[0], X_ik)\n",
    "    \n",
    "    return weighting_dic,X_ik,skip_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def random_batch_glove(batch_size, skip_grams, X_ik, weighting_dic):\n",
    "    \n",
    "    #loop through this skipgram, and change it id  because when sending model, it must number\n",
    "    skip_grams_id = [(word2index[skip_gram[0]], word2index[skip_gram[1]]) for skip_gram in skip_grams]\n",
    "    \n",
    "    #randomly pick \"batch_size\" indexes\n",
    "    number_of_choices = len(skip_grams_id)\n",
    "    random_index = np.random.choice(number_of_choices, batch_size, replace=False) #no repeating indexes among these random indexes\n",
    "    \n",
    "    random_inputs = [] #xi, wi (in batches)\n",
    "    random_labels = [] #xj, wj (in batches)\n",
    "    random_coocs  = [] #Xij (in batches)\n",
    "    random_weighting = [] #f(Xij) (in batches)\n",
    "    #for each of the sample in these indexes\n",
    "    for i in random_index:\n",
    "        random_inputs.append([skip_grams_id[i][0]]) #same reason why i put bracket here....\n",
    "        random_labels.append([skip_grams_id[i][1]])\n",
    "        \n",
    "        #get cooc\n",
    "        #first check whether it exists...\n",
    "        pair = skip_grams[i]  #e.g., ('banana', 'fruit)\n",
    "        try:\n",
    "            cooc = X_ik[pair]\n",
    "        except:\n",
    "            cooc = 1 #label smoothing\n",
    "            \n",
    "        random_coocs.append([math.log(cooc)])  #1. why log, #2, why bracket -> size ==> (, 1)  #my neural network expects (, 1)\n",
    "        \n",
    "        #get weighting\n",
    "        weighting = weighting_dic[pair]  #why not use try....maybe it does not exist....\n",
    "        random_weighting.append(weighting)\n",
    "\n",
    "        \n",
    "    return np.array(random_inputs), np.array(random_labels), np.array(random_coocs), np.array(random_weighting)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Skipgram(nn.Module):\n",
    "    \n",
    "    def __init__(self, voc_size, emb_size):\n",
    "        super(Skipgram, self).__init__()\n",
    "        self.embedding_center_word  = nn.Embedding(voc_size, emb_size)  #is a lookup table mapping all ids in voc_size, into some vector of size emb_size\n",
    "        self.embedding_outside_word = nn.Embedding(voc_size, emb_size)\n",
    "    \n",
    "    def forward(self, center_word, outside_word, all_vocabs):\n",
    "        #center_word, outside_word: (batch_size, 1)\n",
    "        #all_vocabs: (batch_size, voc_size)\n",
    "        \n",
    "        #convert them into embedding\n",
    "        center_word_embed  = self.embedding_center_word(center_word)     #(batch_size, 1, emb_size)\n",
    "        outside_word_embed = self.embedding_outside_word(outside_word)   #(batch_size, 1, emb_size)\n",
    "        all_vocabs_embed   = self.embedding_outside_word(all_vocabs)     #(batch_size, voc_size, emb_size)\n",
    "        \n",
    "        #bmm is basically @ or .dot , but across batches (i.e., ignore the batch dimension)\n",
    "        top_term = outside_word_embed.bmm(center_word_embed.transpose(1, 2)).squeeze(2)\n",
    "        #(batch_size, 1, emb_size) @ (batch_size, emb_size, 1) = (batch_size, 1, 1) ===> (batch_size, 1)\n",
    "        \n",
    "        top_term_exp = torch.exp(top_term)  #exp(uo vc)\n",
    "        #(batch_size, 1)\n",
    "        \n",
    "        lower_term = all_vocabs_embed.bmm(center_word_embed.transpose(1, 2)).squeeze(2)\n",
    "         #(batch_size, voc_size, emb_size) @ (batch_size, emb_size, 1) = (batch_size, voc_size, 1) = (batch_size, voc_size)\n",
    "         \n",
    "        lower_term_sum = torch.sum(torch.exp(lower_term), 1) #sum exp(uw vc)\n",
    "        #(batch_size, 1)\n",
    "        \n",
    "        loss_fn = -torch.mean(torch.log(top_term_exp / lower_term_sum))\n",
    "        #(batch_size, 1) / (batch_size, 1) ==mean==> scalar\n",
    "        \n",
    "        return loss_fn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skipgram Neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipgramNeg(nn.Module):\n",
    "    \n",
    "    def __init__(self, voc_size, emb_size):\n",
    "        super(SkipgramNeg, self).__init__()\n",
    "        self.embedding_center_word  = nn.Embedding(voc_size, emb_size)\n",
    "        self.embedding_outside_word = nn.Embedding(voc_size, emb_size)\n",
    "        self.logsigmoid = nn.LogSigmoid()\n",
    "        \n",
    "    def forward(self, center_words, outside_words, negative_words):\n",
    "        #center_words, outside_words: (batch_size, 1)\n",
    "        #negative_words:  (batch_size, k)\n",
    "        \n",
    "        center_embed  = self.embedding_center_word(center_words)    #(batch_size, 1, emb_size)\n",
    "        outside_embed = self.embedding_outside_word(outside_words)  #(batch_size, 1, emb_size)\n",
    "        neg_embed     = self.embedding_outside_word(negative_words) #(batch_size, k, emb_size)\n",
    "        \n",
    "        uovc          =  outside_embed.bmm(center_embed.transpose(1, 2)).squeeze(2)  #(batch_size, 1)\n",
    "        ukvc          = -neg_embed.bmm(center_embed.transpose(1, 2)).squeeze(2)  #(batch_size, k)\n",
    "        ukvc_sum      =  torch.sum(ukvc, 1).view(-1, 1) #(batch_size, 1)\n",
    "        \n",
    "        loss = self.logsigmoid(uovc) + self.logsigmoid(ukvc_sum)  #(batch_size, 1) + (batch_size, 1)\n",
    "                \n",
    "        return -torch.mean(loss)  #scalar, loss should be scalar, to call backward()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOW\n",
    "same as skipgram"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GloVe(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size,embed_size):\n",
    "        super(GloVe,self).__init__()\n",
    "        self.embedding_center_word = nn.Embedding(vocab_size, embed_size) # center embedding\n",
    "        self.embedding_outside_word = nn.Embedding(vocab_size, embed_size) # out embedding\n",
    "        \n",
    "        self.v_bias = nn.Embedding(vocab_size, 1)\n",
    "        self.u_bias = nn.Embedding(vocab_size, 1)\n",
    "        \n",
    "    def forward(self, center_words, target_words, coocs, weighting):\n",
    "        center_embeds = self.embedding_center_word(center_words) # [batch_size, 1, emb_size]\n",
    "        target_embeds = self.embedding_outside_word(target_words) # [batch_size, 1, emb_size]\n",
    "        \n",
    "        center_bias = self.v_bias(center_words).squeeze(1)\n",
    "        target_bias = self.u_bias(target_words).squeeze(1)\n",
    "        \n",
    "        inner_product = target_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2)\n",
    "        #[batch_size, 1, emb_size] @ [batch_size, emb_size, 1] = [batch_size, 1, 1] = [batch_size, 1]\n",
    "        \n",
    "        #note that coocs already got log\n",
    "        loss = weighting*torch.pow(inner_product +center_bias + target_bias - coocs, 2)\n",
    "        \n",
    "        return torch.sum(loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total word :  25398\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[64], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m learning_rate \u001b[39m=\u001b[39m \u001b[39m0.0001\u001b[39m\n\u001b[0;32m     10\u001b[0m all_vocabs \u001b[39m=\u001b[39m prepare_sequence(\u001b[39mlist\u001b[39m(vocabs), word2index)\u001b[39m.\u001b[39mexpand(batch_size, voc_size)\n\u001b[1;32m---> 11\u001b[0m weighting_dic,X_ik,skip_gram_word \u001b[39m=\u001b[39m wdic_co(vocabs,corpus_tokenized,window_size)\n\u001b[0;32m     12\u001b[0m num_epochs \u001b[39m=\u001b[39m \u001b[39m5000\u001b[39m\n",
      "Cell \u001b[1;32mIn[59], line 34\u001b[0m, in \u001b[0;36mwdic_co\u001b[1;34m(vocabs, corpus_tokenized, window_size)\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[39m#apply the weighting function using this co-occurrence matrix thingy    \u001b[39;00m\n\u001b[0;32m     33\u001b[0m     weighting_dic[bigram] \u001b[39m=\u001b[39m weighting(bigram[\u001b[39m0\u001b[39m], bigram[\u001b[39m1\u001b[39m], X_ik)\n\u001b[1;32m---> 34\u001b[0m     weighting_dic[(bigram[\u001b[39m1\u001b[39m], bigram[\u001b[39m0\u001b[39m])] \u001b[39m=\u001b[39m weighting(bigram[\u001b[39m1\u001b[39;49m], bigram[\u001b[39m0\u001b[39;49m], X_ik)\n\u001b[0;32m     36\u001b[0m \u001b[39mreturn\u001b[39;00m weighting_dic,X_ik,skip_grams\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "window_size = 1\n",
    "unigram_table = unigram(corpus_tokenized,vocabs)\n",
    "skipgram_data = pair_data(corpus_tokenized,mode = \"skipgram\",window_size = window_size)\n",
    "cbow_data = pair_data(corpus_tokenized,mode = \"cbow\",window_size = window_size)\n",
    "voc_size   = len(vocabs)\n",
    "batch_size = 8 \n",
    "emb_size   = 20\n",
    "num_neg = 10\n",
    "learning_rate = 0.0001\n",
    "all_vocabs = prepare_sequence(list(vocabs), word2index).expand(batch_size, voc_size)\n",
    "weighting_dic,X_ik,skip_gram_word = wdic_co(vocabs,corpus_tokenized,window_size)\n",
    "num_epochs = 5000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow_model = Skipgram(voc_size, emb_size)\n",
    "skip_gram_model = Skipgram(voc_size, emb_size)\n",
    "skip_gram_neg_model = SkipgramNeg(voc_size, emb_size) \n",
    "glove_model = GloVe(voc_size, emb_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow_optimizer = optim.Adam(cbow_model.parameters(), lr=learning_rate)\n",
    "Skipgram_optimizer  = optim.Adam(skip_gram_model.parameters(), lr=learning_rate)\n",
    "SkipgramNeg_optimizer  = optim.Adam(skip_gram_neg_model.parameters(), lr=learning_rate)\n",
    "glove_optimizer = optim.Adam(glove_model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trianing loop"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "skip gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000 | Loss: 8.284243 | Time: 1.99 sec\n",
      "Epoch 2000 | Loss: 3.580564 | Time: 1.94 sec\n",
      "Epoch 3000 | Loss: 5.610363 | Time: 1.97 sec\n",
      "Epoch 4000 | Loss: 7.395845 | Time: 1.87 sec\n",
      "Epoch 5000 | Loss: 3.942111 | Time: 1.85 sec\n",
      "total time : 9.63 sec\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "pre_time = start_time\n",
    "\n",
    "#for epoch\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    #get random batch\n",
    "    input_batch, label_batch = random_batch(batch_size, skipgram_data, window_size)\n",
    "    input_batch = torch.LongTensor(input_batch)\n",
    "    label_batch = torch.LongTensor(label_batch)\n",
    "    \n",
    "    # print(input_batch.shape, label_batch.shape, all_vocabs.shape)\n",
    "    \n",
    "    #loss = model\n",
    "    loss = skip_gram_model(input_batch, label_batch, all_vocabs)\n",
    "\n",
    "    #backpropagate\n",
    "    loss.backward()\n",
    "\n",
    "    #update alpha\n",
    "    Skipgram_optimizer.step()\n",
    "    \n",
    "    \n",
    "\n",
    "    #print epoch loss\n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        curr_time = time.time()\n",
    "        print(f\"Epoch {epoch+1} | Loss: {loss:.6f} | Time: {curr_time-pre_time:.2f} sec\")\n",
    "        pre_time = curr_time\n",
    "\n",
    "print(f\"total time : {curr_time-start_time:.2f} sec\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000 | Loss: 10.574606 | Time: 1.94 sec\n",
      "Epoch 2000 | Loss: 7.522035 | Time: 1.85 sec\n",
      "Epoch 3000 | Loss: 5.535345 | Time: 1.85 sec\n",
      "Epoch 4000 | Loss: 6.549340 | Time: 1.85 sec\n",
      "Epoch 5000 | Loss: 5.446101 | Time: 1.85 sec\n",
      "total time : 9.35 sec\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "pre_time = start_time\n",
    "\n",
    "#for epoch\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    #get random batch\n",
    "    input_batch, label_batch = random_batch(batch_size, cbow_data, window_size)\n",
    "    input_batch = torch.LongTensor(input_batch)\n",
    "    label_batch = torch.LongTensor(label_batch)\n",
    "    \n",
    "    # print(input_batch.shape, label_batch.shape, all_vocabs.shape)\n",
    "    \n",
    "    #loss = model\n",
    "    loss = cbow_model(input_batch, label_batch, all_vocabs)\n",
    "\n",
    "    #backpropagate\n",
    "    loss.backward()\n",
    "\n",
    "    #update alpha\n",
    "    cbow_optimizer.step()\n",
    "    \n",
    "    #print epoch loss\n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        curr_time = time.time()\n",
    "        print(f\"Epoch {epoch+1} | Loss: {loss:.6f} | Time: {curr_time-pre_time:.2f} sec\")\n",
    "        pre_time = curr_time\n",
    "\n",
    "print(f\"total time : {curr_time-start_time:.2f} sec\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "skip gram + negative sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000 | Loss: 7.365585 | Time: 2.05 sec\n",
      "Epoch 2000 | Loss: 5.202758 | Time: 2.03 sec\n",
      "Epoch 3000 | Loss: 5.933656 | Time: 1.91 sec\n",
      "Epoch 4000 | Loss: 2.172404 | Time: 2.17 sec\n",
      "Epoch 5000 | Loss: 1.442389 | Time: 1.97 sec\n",
      "total time : 10.13 sec\n"
     ]
    }
   ],
   "source": [
    "#start time\n",
    "start_time = time.time()\n",
    "pre_time = start_time\n",
    "#for epoch\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    #get random batch\n",
    "    input_batch, label_batch = random_batch(batch_size, skipgram_data, window_size)\n",
    "    input_batch = torch.LongTensor(input_batch)\n",
    "    label_batch = torch.LongTensor(label_batch)\n",
    "    neg_batch   = negative_sampling(label_batch, unigram_table,num_neg)    \n",
    "    \n",
    "    #loss = model\n",
    "    loss = skip_gram_neg_model(input_batch, label_batch, neg_batch)\n",
    "\n",
    "    #backpropagate\n",
    "    loss.backward()\n",
    "    \n",
    "    #update alpha\n",
    "    SkipgramNeg_optimizer.step()\n",
    "    \n",
    "    #print epoch loss\n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        curr_time = time.time()\n",
    "        print(f\"Epoch {epoch+1} | Loss: {loss:.6f} | Time: {curr_time-pre_time:.2f} sec\")\n",
    "        pre_time = curr_time\n",
    "\n",
    "print(f\"total time : {curr_time-start_time:.2f} sec\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000 | Loss: 105.895103 | Time: 3.71 sec\n",
      "Epoch 2000 | Loss: 208.897903 | Time: 3.33 sec\n",
      "Epoch 3000 | Loss: 163.534500 | Time: 3.29 sec\n",
      "Epoch 4000 | Loss: 30.706980 | Time: 3.34 sec\n",
      "Epoch 5000 | Loss: 27.159241 | Time: 3.47 sec\n",
      "total time : 17.14 sec\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#start time\n",
    "start_time = time.time()\n",
    "pre_time = start_time\n",
    "#for epoch\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    #get random batch\n",
    "    input, target, cooc, weightin = random_batch_glove(batch_size,skip_grams = skip_gram_word, X_ik= X_ik,weighting_dic = weighting_dic)\n",
    "    input_batch    = torch.LongTensor(input)\n",
    "    target_batch   = torch.LongTensor(target)\n",
    "    cooc_batch     = torch.FloatTensor(cooc)\n",
    "    weightin_batch = torch.FloatTensor(weightin)\n",
    "    \n",
    "    \n",
    "    # print(input_batch.shape, label_batch.shape, cooc_batch.shape, weightin_batch)\n",
    "    \n",
    "    #loss = model\n",
    "    loss = glove_model(input_batch, target_batch, cooc_batch, weightin_batch)\n",
    "    \n",
    "    #backpropagate\n",
    "    loss.backward()\n",
    "    \n",
    "    #update alpha\n",
    "    glove_optimizer.step()\n",
    "    \n",
    "    #print epoch loss\n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        curr_time = time.time()\n",
    "        print(f\"Epoch {epoch+1} | Loss: {loss:.6f} | Time: {curr_time-pre_time:.2f} sec\")\n",
    "        pre_time = curr_time\n",
    "\n",
    "print(f\"total time : {curr_time-start_time:.2f} sec\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### analogy\n",
    "from the paper they said \"We answer the question “a is to b\n",
    "as c is to ?” by finding the word d whose representation wd is closest to wb − wa + wc according\n",
    "to the cosine similaritythey compare word b - word a + word c and find word d by the closest word using cosine similar \". so i just do as they say and count correct answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embed(word,model):\n",
    "    try:\n",
    "        index = word2index[word]\n",
    "    except:\n",
    "        index = word2index['<UNK>']\n",
    "    \n",
    "    word = torch.LongTensor([index])\n",
    "\n",
    "    center_embed  = model.embedding_center_word(word)\n",
    "    outside_embed = model.embedding_outside_word(word)\n",
    "    \n",
    "    embed = (center_embed + outside_embed) / 2\n",
    "    \n",
    "    return  np.array([embed[0][0].item(), embed[0][1].item()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cos_similarity(a,b):\n",
    "\n",
    "    cos_sim = dot(a, b)/(norm(a)*norm(b))\n",
    "\n",
    "    return cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this funtion to find most closest word (inspired by my friend i'm not copy)\n",
    "def similarity_word(wv,vocab_dict):\n",
    "    closest_idx = np.argmax([cos_similarity(wv, v) for v in vocab_dict.values()])\n",
    "    return list(vocab_dict.keys())[closest_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cbow semantic accuracy : 0.89 % \n",
      "Cbow syntactic accuracy : 0.73 % \n",
      "Skip gram semantic accuracy : 0.89 % \n",
      "Skip gram syntactic accuracy : 0.58 % \n",
      "Skip gram Neg semantic accuracy : 0.69 % \n",
      "Skip gram Neg syntactic accuracy : 0.35 % \n",
      "GloVe semantic accuracy : 0.59 % \n",
      "GloVe syntactic accuracy : 0.54 % \n"
     ]
    }
   ],
   "source": [
    "# list of models and names\n",
    "models = [cbow_model,skip_gram_model,skip_gram_neg_model,glove_model]\n",
    "model_name = [\"Cbow\",\"Skip gram\",\"Skip gram Neg\",\"GloVe\"]\n",
    "\n",
    "vocabvec = []\n",
    "# loop model \n",
    "for i in range(len(models)):\n",
    "    model = models[i]\n",
    "    syntactic_accuracy = 0\n",
    "    semantic_accuracy = 0\n",
    "    vocab_dict = {word: get_embed(word,model) for word in vocabs}\n",
    "    vocabvec.append(vocab_dict) \n",
    "\n",
    "    # loop all question for semantic test\n",
    "    for q in analogy[\"semantic\"]:\n",
    "        # Compare embeddings to determine semantic similarity \n",
    "        q_embeddings = [get_embed(word, model) for word in q]\n",
    "\n",
    "        # from paper the answer is word b - word a + word c and find word d by the closest word using cosine similar \n",
    "        word_d = q_embeddings[1] - q_embeddings[0] + q_embeddings[2] \n",
    "\n",
    "        # see the most similarword in vocabs\n",
    "        pred_w = similarity_word(word_d,vocab_dict)\n",
    "\n",
    "        # if predict word is word_d count as correct\n",
    "        if pred_w == q[3]:\n",
    "            semantic_accuracy += 1\n",
    "\n",
    "    semantic_accuracy /= len(analogy[\"semantic\"])\n",
    "    print(f\"{model_name[i]} semantic accuracy : {semantic_accuracy*100:.2f} % \" )\n",
    "\n",
    "    # loop all question for syntactic test\n",
    "    for q in analogy[\"syntactic\"]:\n",
    "        q_embeddings = [get_embed(word, model) for word in q]\n",
    "        # Compare embeddings to determine syntactic similarity (can use cosine similarity)\n",
    "        word_d = q_embeddings[1] - q_embeddings[0] + q_embeddings[2] \n",
    "\n",
    "        pred_w = similarity_word(word_d,vocab_dict)\n",
    "\n",
    "        # if predict word is word_d count as correct\n",
    "        if pred_w == q[3]:\n",
    "            syntactic_accuracy += 1\n",
    "\n",
    "    syntactic_accuracy /= len(analogy[\"syntactic\"])\n",
    "    print(f\"{model_name[i]} syntactic accuracy : {syntactic_accuracy*100:.2f} % \" )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "most are incorrect , am i do something wrong lol"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correration \n",
    "in the paper they use spearman methode i guese it is spearmanr from spicy.state "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The correlation between the predicted and human similarity of Cbow is: 0.72\n",
      "The correlation between the predicted and human similarity of Skip gram is: -0.42\n",
      "The correlation between the predicted and human similarity of Skip gram Neg is: 0.67\n",
      "The correlation between the predicted and human similarity of GloVe is: 0.75\n",
      "3.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "# Load the dataset into a DataFrame\n",
    "data = pd.read_csv(\"wordsim_similarity_goldstandard.txt\", sep='\\t', names=[\"word1\", \"word2\", \"human_score\"])\n",
    "\n",
    "# Use the model to predict similarity scores for each word pair\n",
    "# and store the predicted scores in a new column\n",
    "# vocab is the list of words in your vocabulary\n",
    "# list of models and names\n",
    "models = [cbow_model,skip_gram_model,skip_gram_neg_model,glove_model]\n",
    "model_name = [\"Cbow\",\"Skip gram\",\"Skip gram Neg\",\"GloVe\"]\n",
    "count = 0\n",
    "for i in range(4):\n",
    "    similarities = []\n",
    "    human_scores = []\n",
    "    \n",
    "    # data is the dataframe containing the word pairs and human-assigned similarity scores\n",
    "    for index,row in data.iterrows():\n",
    "        word1 = row[\"word1\"]\n",
    "        word2 = row[\"word2\"]\n",
    "        human_score = row[\"human_score\"]\n",
    "        # check if wordsim word appear in vocabs\n",
    "        if word1 in vocabs and word2 in vocabs:\n",
    "            # convert the word to its corresponding word embeddings using the trained GloVe model\n",
    "            count +=1\n",
    "\n",
    "            center_embeds = get_embed(word1,models[i]) # [batch_size, 1, emb_size]\n",
    "            target_embeds = get_embed(word2,models[i]) # [batch_size, 1, emb_size]\n",
    "            #             calculate the cosine similarity between the embeddings\n",
    "            similarity = cos_similarity(center_embeds,target_embeds)\n",
    "            similarities.append(similarity)\n",
    "            human_scores.append(human_score)\n",
    "\n",
    "    # now you can calculate the correlation between the predicted similarity scores and the human-assigned similarity scores\n",
    "    correlation = pearsonr(human_scores, similarities)[0]\n",
    "    print(f\"The correlation between the predicted and human similarity of {model_name[i]} is: {correlation:.2f}\")\n",
    "\n",
    "# how many time\n",
    "print(count/4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "12b51c3a15c6d04bbe25e0ed0a8589f4bf8f73b3e40dc1f9d202d81fcb7450ab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
